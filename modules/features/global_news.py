import httpx
from bs4 import BeautifulSoup
from modules.nlp.openai_utils import summarize_with_gpt

async def get_global_news(limit: int = 3) -> str:
    """
    ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏à‡∏≤‡∏Å Google News RSS ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡∏∏‡∏õ‡∏î‡πâ‡∏ß‡∏¢ GPT ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÅ‡∏ö‡∏ö‡∏¢‡πà‡∏≠
    """
    url = "https://news.google.com/rss/search?q=‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®&hl=th&gl=TH&ceid=TH:th"

    try:
        async with httpx.AsyncClient() as client:
            res = await client.get(url, timeout=10)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, "xml")

            items = soup.find_all("item", limit=limit)
            if not items:
                return "‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÉ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ"

            summarized_news = []

            for item in items:
                title = item.title.text.strip()
                link = item.link.text.strip() if item.link else ""
                raw_desc = item.description.text if item.description else ""
                clean_desc = BeautifulSoup(raw_desc, "html.parser").get_text()

                # ‡∏£‡∏ß‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÅ‡∏•‡∏∞‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡∏∏‡∏õ
                full_text = f"{title}\n{clean_desc}"
                summary = await summarize_with_gpt(full_text)

                news_block = f"üåç {summary}"
                if link:
                    news_block += f"\nüîó [‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠](<{link}>)"

                summarized_news.append(news_block)

            return "üåê ‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÄ‡∏î‡πà‡∏ô‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ:\n\n" + "\n\n".join(summarized_news)

    except Exception as e:
        return f"‚ùå ‡∏û‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ({e})"
